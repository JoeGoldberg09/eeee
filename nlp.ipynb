{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMobMhS2ayNbxlawlmAYUNW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoeGoldberg09/eeee/blob/main/nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One hot encoding\n"
      ],
      "metadata": {
        "id": "k9urTN0ULYF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read three text files\n",
        "filenames = ['tech1.txt', 'tech2.txt', 'tech3.txt']\n",
        "texts = []\n",
        "for filename in filenames:\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        texts.append(file.read())\n",
        "\n",
        "# Split texts into sentences\n",
        "sentences = []\n",
        "for text in texts:\n",
        "    sentences.extend(text.split('.'))\n",
        "\n",
        "sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "# Create vocabulary of unique words\n",
        "vocab = set()\n",
        "for sentence in sentences:\n",
        "    words = sentence.split()\n",
        "    vocab.update(words)\n",
        "\n",
        "vocab = sorted(list(vocab))  # To keep order consistent\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "# Create one-hot encoding for each sentence\n",
        "one_hot_encodings = []\n",
        "for sentence in sentences:\n",
        "    encoding = [0] * len(vocab)\n",
        "    words = sentence.split()\n",
        "    for word in words:\n",
        "        if word in word_to_index:\n",
        "            encoding[word_to_index[word]] = 1\n",
        "    one_hot_encodings.append(encoding)\n",
        "\n",
        "# Display sample\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"\\nFirst sentence:\", sentences[0])\n",
        "print(\"One-hot encoding for first sentence:\", one_hot_encodings[0])\n"
      ],
      "metadata": {
        "id": "JoufKBhQLkke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words"
      ],
      "metadata": {
        "id": "ufBiuSKYLlid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read three text files\n",
        "filenames = ['movie1.txt', 'movie2.txt', 'movie3.txt']\n",
        "texts = []\n",
        "for filename in filenames:\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        texts.append(file.read())\n",
        "\n",
        "# Create vocabulary\n",
        "vocab = set()\n",
        "for text in texts:\n",
        "    words = text.split()\n",
        "    vocab.update(words)\n",
        "\n",
        "vocab = sorted(list(vocab))\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "# Create Bag of Words for each document\n",
        "bow_matrix = []\n",
        "for text in texts:\n",
        "    bow_vector = [0] * len(vocab)\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "        if word in word_to_index:\n",
        "            bow_vector[word_to_index[word]] += 1\n",
        "    bow_matrix.append(bow_vector)\n",
        "\n",
        "# Display sample\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"\\nBag of Words matrix:\")\n",
        "for vector in bow_matrix:\n",
        "    print(vector)\n"
      ],
      "metadata": {
        "id": "m4kpyalELqaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF\n"
      ],
      "metadata": {
        "id": "AANVRo0kLnSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "import re\n",
        "\n",
        "def read_file(filepath):\n",
        "    \"\"\"Reads a file and returns its content as a string.\"\"\"\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        return f.read()\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"Tokenizes the text into lowercase words, removing punctuation.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove punctuation\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "def compute_tf(tokens):\n",
        "    \"\"\"Computes term frequency for a list of tokens.\"\"\"\n",
        "    tf = {}\n",
        "    total_tokens = len(tokens)\n",
        "    for word in tokens:\n",
        "        tf[word] = tf.get(word, 0) + 1\n",
        "    for word in tf:\n",
        "        tf[word] /= total_tokens  # Normalize\n",
        "    return tf\n",
        "\n",
        "def compute_idf(documents):\n",
        "    \"\"\"Computes inverse document frequency for all tokens in all documents.\"\"\"\n",
        "    idf = {}\n",
        "    total_documents = len(documents)\n",
        "    all_tokens = set([word for tokens in documents for word in tokens])\n",
        "\n",
        "    for word in all_tokens:\n",
        "        containing_docs = sum(1 for tokens in documents if word in tokens)\n",
        "        idf[word] = math.log(total_documents / (1 + containing_docs)) + 1  # +1 to avoid division by zero\n",
        "    return idf\n",
        "\n",
        "def compute_tf_idf(tf, idf):\n",
        "    \"\"\"Computes TF-IDF for a document.\"\"\"\n",
        "    tf_idf = {}\n",
        "    for word, tf_value in tf.items():\n",
        "        tf_idf[word] = tf_value * idf.get(word, 0)\n",
        "    return tf_idf\n",
        "\n",
        "def print_top_terms(tf_idf, top_n=10):\n",
        "    \"\"\"Prints top N terms with highest TF-IDF scores.\"\"\"\n",
        "    sorted_terms = sorted(tf_idf.items(), key=lambda item: item[1], reverse=True)\n",
        "    for word, score in sorted_terms[:top_n]:\n",
        "        print(f\"{word}: {score:.4f}\")\n",
        "    print()\n",
        "\n",
        "def main():\n",
        "    # Assuming you have 3 text files: 'place1.txt', 'place2.txt', 'place3.txt'\n",
        "    filepaths = ['place1.txt', 'place2.txt', 'place3.txt']\n",
        "\n",
        "    documents = [tokenize(read_file(filepath)) for filepath in filepaths]\n",
        "\n",
        "    # Compute TF for each document\n",
        "    tfs = [compute_tf(tokens) for tokens in documents]\n",
        "\n",
        "    # Compute IDF using all documents\n",
        "    idf = compute_idf(documents)\n",
        "\n",
        "    # Compute TF-IDF for each document\n",
        "    tf_idfs = [compute_tf_idf(tf, idf) for tf in tfs]\n",
        "\n",
        "    # Print top 10 terms per document\n",
        "    for i, tf_idf in enumerate(tf_idfs):\n",
        "        print(f\"Top terms for Document {i+1}:\")\n",
        "        print_top_terms(tf_idf)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "PR93nGqMLuzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text cleaning by removing punctuation/special characters, numbers\n",
        "and extra white spaces. Use regular expression for the same.\n",
        "Convert text to lowercase\n",
        "Tokenization\n",
        "Remove stop words\n",
        "Correct misspelled words"
      ],
      "metadata": {
        "id": "qT66ImW_L9NN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# --- Functions ---\n",
        "\n",
        "def read_file(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove punctuation, numbers\n",
        "    text = re.sub(r'\\s+', ' ', text)          # Remove extra white spaces\n",
        "    return text.lower()                      # Convert to lowercase\n",
        "\n",
        "def tokenize(text):\n",
        "    return nltk.word_tokenize(text)\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "def correct_spelling(tokens):\n",
        "    corrected = []\n",
        "    for word in tokens:\n",
        "        blob = TextBlob(word)\n",
        "        corrected.append(str(blob.correct()))\n",
        "    return corrected\n",
        "\n",
        "# --- Main Program ---\n",
        "\n",
        "text = read_file('your_text_file.txt')\n",
        "cleaned_text = clean_text(text)\n",
        "tokens = tokenize(cleaned_text)\n",
        "filtered_tokens = remove_stopwords(tokens)\n",
        "final_tokens = correct_spelling(filtered_tokens)\n",
        "\n",
        "print(final_tokens)\n"
      ],
      "metadata": {
        "id": "Kcgq4JeQL687"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text cleaning by removing punctuation/special characters, numbers\n",
        "and extra white spaces. Use regular expression for the same.\n",
        "Convert text to lowercase\n",
        "Stemming and Lemmatization\n",
        "Create a list of 3 consecutive words after lemmatization"
      ],
      "metadata": {
        "id": "h9KYIAbIMnfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import ngrams\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def read_file(file_path):\n",
        "    \"\"\"Read text from a file\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Remove punctuation, special characters, numbers, and extra white spaces\"\"\"\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove extra white spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def convert_to_lowercase(text):\n",
        "    \"\"\"Convert text to lowercase\"\"\"\n",
        "    return text.lower()\n",
        "\n",
        "def tokenize_text(text):\n",
        "    \"\"\"Tokenize text into sentences and words\"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text)\n",
        "    return sentences, words\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stop words from the list of words\"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return filtered_words\n",
        "\n",
        "def correct_spelling(words):\n",
        "    \"\"\"Correct misspelled words\"\"\"\n",
        "    spell = SpellChecker()\n",
        "    corrected_words = []\n",
        "\n",
        "    for word in words:\n",
        "        # Find those words that may be misspelled\n",
        "        if word in spell:\n",
        "            corrected_words.append(word)\n",
        "        else:\n",
        "            # Get the most likely correction\n",
        "            corrected_word = spell.correction(word)\n",
        "            if corrected_word:\n",
        "                corrected_words.append(corrected_word)\n",
        "            else:\n",
        "                corrected_words.append(word)\n",
        "\n",
        "    return corrected_words\n",
        "\n",
        "def perform_stemming(words):\n",
        "    \"\"\"Perform stemming on words\"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    return stemmed_words\n",
        "\n",
        "def perform_lemmatization(words):\n",
        "    \"\"\"Perform lemmatization on words\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
        "    return lemmatized_words\n",
        "\n",
        "def create_trigrams(words):\n",
        "    \"\"\"Create a list of 3 consecutive words (trigrams)\"\"\"\n",
        "    return list(ngrams(words, 3))\n",
        "\n",
        "def main():\n",
        "    # File path - replace with your actual file path\n",
        "    file_path = \"sample_text.txt\"\n",
        "\n",
        "    # Read the file\n",
        "    text = read_file(file_path)\n",
        "    print(\"Original text (first 200 characters):\")\n",
        "    print(text[:200] + \"...\\n\")\n",
        "\n",
        "    # Task a: Text cleaning\n",
        "    cleaned_text = clean_text(text)\n",
        "    print(\"Text after cleaning (first 200 characters):\")\n",
        "    print(cleaned_text[:200] + \"...\\n\")\n",
        "\n",
        "    # Task b: Convert to lowercase\n",
        "    lowercase_text = convert_to_lowercase(cleaned_text)\n",
        "    print(\"Text after converting to lowercase (first 200 characters):\")\n",
        "    print(lowercase_text[:200] + \"...\\n\")\n",
        "\n",
        "    # Task c: Tokenization\n",
        "    sentences, words = tokenize_text(lowercase_text)\n",
        "    print(f\"Number of sentences: {len(sentences)}\")\n",
        "    print(f\"Number of words: {len(words)}\")\n",
        "    print(\"First 10 words:\")\n",
        "    print(words[:10])\n",
        "    print()\n",
        "\n",
        "    # Task d: Remove stop words\n",
        "    filtered_words = remove_stopwords(words)\n",
        "    print(f\"Number of words after removing stop words: {len(filtered_words)}\")\n",
        "    print(\"First 10 words after removing stop words:\")\n",
        "    print(filtered_words[:10])\n",
        "    print()\n",
        "\n",
        "    # Task e: Correct misspelled words\n",
        "    corrected_words = correct_spelling(filtered_words)\n",
        "    print(\"First 10 words after spell correction:\")\n",
        "    print(corrected_words[:10])\n",
        "    print()\n",
        "\n",
        "    # Stemming\n",
        "    stemmed_words = perform_stemming(corrected_words)\n",
        "    print(\"First 10 words after stemming:\")\n",
        "    print(stemmed_words[:10])\n",
        "    print()\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatized_words = perform_lemmatization(corrected_words)\n",
        "    print(\"First 10 words after lemmatization:\")\n",
        "    print(lemmatized_words[:10])\n",
        "    print()\n",
        "\n",
        "    # Create trigrams\n",
        "    trigrams = create_trigrams(lemmatized_words)\n",
        "    print(\"First 5 trigrams after lemmatization:\")\n",
        "    print(trigrams[:5])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "IpJmlvuYMn0k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}